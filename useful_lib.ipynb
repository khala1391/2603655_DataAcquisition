{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3bf642d",
   "metadata": {},
   "source": [
    "## Module list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca02e3b1-6983-42eb-b25c-7ccb0b043626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.python.org/3/library/index.html\n",
    "\n",
    "# https://www.w3schools.com/python/python_ref_functions.asp\n",
    "\n",
    "# text processing services\n",
    "import re\n",
    "# data types\n",
    "import datetime\n",
    "import pprint\n",
    "# numeric and maths\n",
    "import random\n",
    "# functional programming\n",
    "import itertools\n",
    "import functools\n",
    "# file and directory access\n",
    "import pathlib\n",
    "import os.path\n",
    "# data persistence\n",
    "import pickle\n",
    "# data compression and archiving\n",
    "import zipfile\n",
    "# generic operating system\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "# internet data handling\n",
    "import json\n",
    "import base64\n",
    "# structured markup processing tool\n",
    "import xml.etree.ElementTree\n",
    "import xml.dom.minidom\n",
    "# internet protocols and support\n",
    "import webbrowser\n",
    "import urllib.request\n",
    "# debugging and profiling\n",
    "import timeit\n",
    "# python runtime services\n",
    "import sys\n",
    "\n",
    "# https://pypi.org/project/pytz/\n",
    "import pytz\n",
    "\n",
    "# https://lxml.de/tutorial.html\n",
    "from lxml import etree\n",
    "\n",
    "# https://ipython.readthedocs.io/en/8.16.0/api/generated/IPython.display.html\n",
    "from IPython.display import Image\n",
    "\n",
    "import chromadb\n",
    "\n",
    "import pythainlp\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4ff273",
   "metadata": {},
   "source": [
    "### JSON dumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ce2e213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"test1\": {\n",
      "        \"email\": \"angela@gmail.com\",\n",
      "        \"password\": \"!b(BVRZ9m$vr+8\"\n",
      "    },\n",
      "    \"test2\": {\n",
      "        \"email\": \"angela@gmail.com\",\n",
      "        \"password\": \"eH12GA##5&vIRg!b(BVRZ9m$vr+8\"\n",
      "    },\n",
      "    \"test3\": {\n",
      "        \"email\": \"angela@gmail.com\",\n",
      "        \"password\": \"#wpzRt#yFQ5E3\"\n",
      "    },\n",
      "    \"testn\": {\n",
      "        \"email\": \"angela@gmail.com\",\n",
      "        \"password\": \"fy(0(Ek%9BQY$h\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "## read json file\n",
    "#1 load\n",
    "#2 dumps (json to strinng)\n",
    "\n",
    "with open('data/example.json') as f:\n",
    "  data_json = json.load(f)\n",
    "\n",
    "print(json.dumps(data_json,indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec261e10",
   "metadata": {},
   "source": [
    "### pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afedd620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'test1': {   'email': 'angela@gmail.com',\n",
      "                 'password': '!b(BVRZ9m$vr+8'},\n",
      "    'test2': {   'email': 'angela@gmail.com',\n",
      "                 'password': 'eH12GA##5&vIRg!b(BVRZ9m$vr+8'},\n",
      "    'test3': {   'email': 'angela@gmail.com',\n",
      "                 'password': '#wpzRt#yFQ5E3'},\n",
      "    'testn': {   'email': 'angela@gmail.com',\n",
      "                 'password': 'fy(0(Ek%9BQY$h'}}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(data_json, indent=4, width=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae761103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"test1\": {\n",
      "        \"email\": \"angela@gmail.com\",\n",
      "        \"password\": \"!b(BVRZ9m$vr+8\"\n",
      "    },\n",
      "    \"test2\": {\n",
      "        \"email\": \"angela@gmail.com\",\n",
      "        \"password\": \"eH12GA##5&vIRg!b(BVRZ9m$vr+8\"\n",
      "    },\n",
      "    \"test3\": {\n",
      "        \"email\": \"angela@gmail.com\",\n",
      "        \"password\": \"#wpzRt#yFQ5E3\"\n",
      "    },\n",
      "    \"testn\": {\n",
      "        \"email\": \"angela@gmail.com\",\n",
      "        \"password\": \"fy(0(Ek%9BQY$h\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "data_dict = json.dumps(data_json,indent=4)\n",
    "print(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df520c05",
   "metadata": {},
   "source": [
    "### pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55da8d1f",
   "metadata": {},
   "source": [
    "- create path object: pathlib.path('...')\n",
    "- join path to file: <path> / <filename.xxx>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1d11787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file name new_directory\\sample_file.txt exists\n",
      "test create new file\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import os.path\n",
    "\n",
    "path = pathlib.Path('new_directory')\n",
    "\n",
    "if not path.exists():\n",
    "    path.mkdir()      # create path\n",
    "\n",
    "file_path = path / 'sample_file.txt'   # join path with file name\n",
    "with open(file_path, 'w') as f:\n",
    "    f.write(\"test create new file\")\n",
    "\n",
    "if os.path.isfile(file_path):    # check file exist or not\n",
    "    print(f\"file name {file_path} exists\")\n",
    "    with open(file_path) as f:\n",
    "        print(f.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a6bfe71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "zzzzz\n"
     ]
    }
   ],
   "source": [
    "# path = pathlib.Path('')\n",
    "path = pathlib.Path()     # home path\n",
    "print(path)\n",
    "\n",
    "file_path = path / 'note.txt'\n",
    "with open(file_path) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834b6f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_directory\\new_subdirectory\n",
      "this is sample in sub\n"
     ]
    }
   ],
   "source": [
    "# path = pathlib.Path('')\n",
    "path = pathlib.Path('new_directory/new_subdirectory')\n",
    "print(path)  # output as OS\n",
    "\n",
    "file_path = path / 'sample_insub.txt'   # '/' ==> better join, ignore OS\n",
    "with open(file_path) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c569a84",
   "metadata": {},
   "source": [
    "#### read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfccd2b",
   "metadata": {},
   "source": [
    "- read()\t\n",
    "  - A single string of all content\t\n",
    "  - For reading small files entirely at once\n",
    "- readline()\t\n",
    "  - A single line at a time\t\n",
    "  - For reading large files line-by-line\n",
    "- readlines()\t\n",
    "  - A list of all lines\t\n",
    "  - For processing all lines in list form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2538fa8",
   "metadata": {},
   "source": [
    "### zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23f4cc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60d6020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with zipfile.ZipFile('data/blognone.zip', 'w', compression =zipfile.ZIP_DEFLATED) as zipf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba83630",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('gitignore/example.zip', 'w') as zipf:\n",
    "    zipf.write('gitignore/Syllabus67.1 2603655 v2.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d565e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('gitignore/example2.zip', 'w') as zipf:\n",
    "    zipf.write('gitignore/test1.txt')\n",
    "    zipf.write('gitignore/test2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e2a634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipfile.writestr('<filename>', stringdata)\n",
    "with zipfile.ZipFile('gitignore/example3.zip', 'w') as zipf:\n",
    "    # Write a string to a file inside the zip\n",
    "    zipf.writestr('file1.txt', 'This is the content of file1.')\n",
    "    # Write another string to a different file\n",
    "    zipf.writestr('file2.txt', 'This is the content of file2.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641cc095",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('gitignore/example2.zip', 'r') as zipf:\n",
    "    zipf.extractall('gitignore/DestinationFolder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f5baf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gitignore/test1.txt', 'gitignore/test2.txt']\n"
     ]
    }
   ],
   "source": [
    "with zipfile.ZipFile('gitignore/example2.zip', 'r') as zipf:\n",
    "    print(zipf.namelist())   # list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217b3e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'zip file test1'\n"
     ]
    }
   ],
   "source": [
    "with zipfile.ZipFile('gitignore/example2.zip', 'r') as zipf:\n",
    "    with zipf.open('gitignore/test1.txt') as file:\n",
    "        content = file.read()\n",
    "        print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d89749",
   "metadata": {},
   "source": [
    "### directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ef0452",
   "metadata": {},
   "source": [
    "#### directory by defined function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54def4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def print_directory_tree(root, indent=\"\"):\n",
    "    root_path = Path(root)\n",
    "    items = sorted(root_path.iterdir(), key=lambda x: (x.is_file(), x.name))\n",
    "    \n",
    "    for i, item in enumerate(items):\n",
    "        connector = \"└──\" if i == len(items) - 1 else \"├──\"\n",
    "        print(f\"{indent}{connector} {item.name}\")\n",
    "        \n",
    "        if item.is_dir():\n",
    "            new_indent = indent + (\"    \" if i == len(items) - 1 else \"│   \")\n",
    "            print_directory_tree(item, new_indent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ea9595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "├── .ipynb_checkpoints\n",
      "│   ├── Perplex01_List-checkpoint.ipynb\n",
      "│   ├── Perplex02_Set-checkpoint.ipynb\n",
      "│   ├── Perplex03_Dictionary-checkpoint.ipynb\n",
      "│   ├── Perplex04_Map_Zip-checkpoint.ipynb\n",
      "│   ├── Perplex05_Pandas_beginner-checkpoint.ipynb\n",
      "│   ├── Perplex06_Pandas_advanced-checkpoint.ipynb\n",
      "│   ├── Perplex07_DataVis-checkpoint.ipynb\n",
      "│   ├── Perplex08_DataCleaning_intro-checkpoint.ipynb\n",
      "│   ├── Perplex09_MissingValue-checkpoint.ipynb\n",
      "│   ├── Perplex10_Outlier-checkpoint.ipynb\n",
      "│   ├── Perplex11_Standardization-checkpoint.ipynb\n",
      "│   ├── Perplex12_DimensionReduction-checkpoint.ipynb\n",
      "│   └── Perplex13_Serialization-checkpoint.ipynb\n",
      "├── Perplex01_List.ipynb\n",
      "├── Perplex02_Set.ipynb\n",
      "├── Perplex03_Dictionary.ipynb\n",
      "├── Perplex04_Map_Zip.ipynb\n",
      "├── Perplex05_Pandas_beginner.ipynb\n",
      "├── Perplex06_Pandas_advanced.ipynb\n",
      "├── Perplex07_DataVis.ipynb\n",
      "├── Perplex08_DataCleaning_intro.ipynb\n",
      "├── Perplex09_MissingValue.ipynb\n",
      "├── Perplex10_Outlier.ipynb\n",
      "├── Perplex11_Standardization.ipynb\n",
      "├── Perplex12_DimensionReduction.ipynb\n",
      "└── Perplex13_Serialization.ipynb\n"
     ]
    }
   ],
   "source": [
    "print_directory_tree(\"perplexity\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991c848d",
   "metadata": {},
   "source": [
    "#### `directory_tree`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e420550",
   "metadata": {},
   "source": [
    "https://pypi.org/project/directory-tree/\n",
    "\n",
    "DisplayTree(\n",
    "    dirPath: str='',\n",
    "    stringRep: bool=False,\n",
    "    header: bool=False,\n",
    "    maxDepth: float=float('inf'),\n",
    "    showHidden: bool=False,\n",
    "    ignoreList: List[str]=None,\n",
    "    onlyFiles: bool=False,\n",
    "    onlyDirs: bool=False,\n",
    "    sortBy: int=0\n",
    ") -> Union[str, None]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b680765c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity/\n",
      "├── Perplex01_List.ipynb\n",
      "├── Perplex02_Set.ipynb\n",
      "├── Perplex03_Dictionary.ipynb\n",
      "├── Perplex04_Map_Zip.ipynb\n",
      "├── Perplex05_Pandas_beginner.ipynb\n",
      "├── Perplex06_Pandas_advanced.ipynb\n",
      "├── Perplex07_DataVis.ipynb\n",
      "├── Perplex08_DataCleaning_intro.ipynb\n",
      "├── Perplex09_MissingValue.ipynb\n",
      "├── Perplex10_Outlier.ipynb\n",
      "├── Perplex11_Standardization.ipynb\n",
      "├── Perplex12_DimensionReduction.ipynb\n",
      "└── Perplex13_Serialization.ipynb\n"
     ]
    }
   ],
   "source": [
    "from directory_tree import DisplayTree\n",
    "DisplayTree(\"perplexity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1abdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/\n",
      "├── aansnook_books.json\n",
      "├── blognone.json\n",
      "├── blognone.zip\n",
      "├── BOW Vect.txt\n",
      "├── bread_production_data.csv\n",
      "├── cartoon1.jpg\n",
      "├── cartoon2.jpg\n",
      "├── churn_data.csv\n",
      "├── content.txt\n",
      "├── csv_json_trans.csv\n",
      "├── csv_json_trans.csv.zip\n",
      "├── csv_trans_tab.csv\n",
      "├── custom_words.json\n",
      "├── data.xml\n",
      "├── eurofxref-daily.xml\n",
      "├── example.json\n",
      "├── example_xml.xml\n",
      "├── historyExchange5Days.zip\n",
      "├── house_data.csv\n",
      "├── json0.json\n",
      "├── json1.json\n",
      "├── json2.json\n",
      "├── json3.json\n",
      "├── json4.json\n",
      "├── khaosod.json\n",
      "├── khaosod.zip\n",
      "├── LLR.txt\n",
      "├── lumphini_park.jpg\n",
      "├── persons.json\n",
      "├── persons.xml\n",
      "├── persons0.json\n",
      "├── plot_count_tfidf.txt\n",
      "├── regex example.csv\n",
      "├── restaurant_ratings.csv\n",
      "├── smart_city.csv\n",
      "├── test.html\n",
      "├── thaiglobal_logistics_data.csv\n",
      "├── thaiglobal_logistics_data_rev2.csv\n",
      "├── thaisky_airways_bookings.csv\n",
      "├── tham_luang_rescue_data.csv\n",
      "├── tham_luang_rescue_data.xlsx\n",
      "├── tips.csv\n",
      "├── traffic_data.csv\n",
      "├── ultra_easy_vector_db.txt\n",
      "└── weather655.xml\n"
     ]
    }
   ],
   "source": [
    "DisplayTree('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6569f56",
   "metadata": {},
   "source": [
    "### format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84fb4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 1\n",
    "end = 100\n",
    "step = 10\n",
    "\n",
    "for i in range(start, end + 1, step):\n",
    "    range_label = f\"{str(i).zfill(3)}-{str(min(i + step - 1, end)).zfill(3)}\"\n",
    "    print(range_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24098b37",
   "metadata": {},
   "source": [
    "### to_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e56800",
   "metadata": {},
   "source": [
    "#### `df.to_json`(orient='table)\n",
    "- in order to open json witht not simple json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af23ebd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>รีวิว ASUS ROG Ally X เทียบ ROG Ally รุ่นแรก ค...</td>\n",
       "      <td>ในช่วง 2-3 ปีที่ผ่านมา วงการเกมมีความเคลื่อนไห...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Meta นำ Facial Recognition มาใช้งานอีกครั้งหลั...</td>\n",
       "      <td>Meta เตรียมนำระบบรู้จำใบหน้าหรือ Facial Recogn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Google เตรียมปิดการทำงานกล่อง Search ย่อยเจาะจ...</td>\n",
       "      <td>กูเกิลแจ้งการปิดการทำงานกล่องค้นหาย่อยในผลค้นห...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  รีวิว ASUS ROG Ally X เทียบ ROG Ally รุ่นแรก ค...   \n",
       "1  Meta นำ Facial Recognition มาใช้งานอีกครั้งหลั...   \n",
       "2  Google เตรียมปิดการทำงานกล่อง Search ย่อยเจาะจ...   \n",
       "\n",
       "                                             content  \n",
       "0  ในช่วง 2-3 ปีที่ผ่านมา วงการเกมมีความเคลื่อนไห...  \n",
       "1  Meta เตรียมนำระบบรู้จำใบหน้าหรือ Facial Recogn...  \n",
       "2  กูเกิลแจ้งการปิดการทำงานกล่องค้นหาย่อยในผลค้นห...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# df_blognone = pd.read_json('data/blognone.json')\n",
    "df_blognone = pd.read_json('data/blognone.json', orient ='table')\n",
    "df_blognone.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa762ee2",
   "metadata": {},
   "source": [
    "#### `df.to_json`(force_acsii)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6aa505",
   "metadata": {},
   "source": [
    "- `force_ascii` = True\n",
    "  - for convert non-ascii (thai font) to ascii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14829c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"column1\":{\"0\":\"value1\",\"1\":\"\\\\u0e04\\\\u0e48\\\\u0e32\\\\u0e43\\\\u0e0a\\\\u0e49\\\\u0e08\\\\u0e48\\\\u0e32\\\\u0e22\"},\"column2\":{\"0\":1,\"1\":2}}'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'column1': ['value1', 'ค่าใช้จ่าย'], 'column2': [1, 2]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert to JSON with force_ascii=True\n",
    "json_data = df.to_json(force_ascii=True)\n",
    "json_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eab5bdb",
   "metadata": {},
   "source": [
    "- `force_ascii` = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc26b66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_corpus.to_json('data/khaosod.json',orient='table',force_ascii=False,indent=2)  \n",
    "\n",
    "# title\tcontent\n",
    "# 0\tหลาก&หลายไอที - อลังการไลน์อัพโน้ตบุ๊กAI เลอโ...\tเลอโนโวเปิดตัวผลิตภัณฑ์ใหม่ล่าสุดของไลน์อัพ Le..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a305aec4",
   "metadata": {},
   "source": [
    "### BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da22596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# html_string = html_file.read().decode('utf-8')\n",
    "# html_soup = BeautifulSoup(html_string,'html.parser')\n",
    "\n",
    "# soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f508e90",
   "metadata": {},
   "source": [
    "element = soup.find(name, attrs, recursive, string, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf783b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = soup.find('a')     # tagname\n",
    "result = soup.find('div', class_='container') # tagname and class\n",
    "result = soup.find('a', href='/about') # tagname and attribute\n",
    "result = soup.find('p', string='Hello World')  # tagname and specific string\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdd6ccf",
   "metadata": {},
   "source": [
    "# PythaiNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd5721c",
   "metadata": {},
   "source": [
    "## word_tokenize\n",
    "- ตัดคำ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d367f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['สวัสดี', 'ครับ']\n"
     ]
    }
   ],
   "source": [
    "from pythainlp.tokenize import word_tokenize\n",
    "\n",
    "text = \"สวัสดีครับ\"\n",
    "words = word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e950d9",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "- create tokenizer object from class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130779fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['สวัสดี', 'ครับ']\n"
     ]
    }
   ],
   "source": [
    "# https://pythainlp.org/docs/4.0/\n",
    "from pythainlp.tokenize import Tokenizer\n",
    "\n",
    "# tokenizer = Tokenizer(custom_dict=custom_words,engine='newmm')\n",
    "tokenizer = Tokenizer()\n",
    "text = \"สวัสดีครับ\"\n",
    "words = tokenizer.word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de95ae88",
   "metadata": {},
   "source": [
    "## Error\n",
    "https://www.w3schools.com/python/python_ref_exceptions.asp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73e0fb5",
   "metadata": {},
   "source": [
    "## Keyword\n",
    "https://www.w3schools.com/python/python_ref_keywords.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77a0b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.tokenize import word_tokenize    # word_tokenize  -> split thai text to word\n",
    "from pythainlp.tokenize import Tokenizer        # Tokenizer -> create reusable tokenizer object\n",
    "\n",
    "\n",
    "from pythainlp.corpus import thai_words         # thai_words -> get word\n",
    "\n",
    "\n",
    "\n",
    "from nltk import ngrams         # ngrams -> generate ngrams as specified number\n",
    "\n",
    "\n",
    "\n",
    "from collections import Counter     # Counter -> count element in hashable object\n",
    "\n",
    "\n",
    "from pythainlp.util import dict_trie    # dict_trie -> create prefix tree structure\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # count word (bow) and put in matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # put in matri of TF-IDF\n",
    "\n",
    "\n",
    "# step for tokenize bigram by LLR\n",
    "# - generate 2-grams\n",
    "# - count occurence\n",
    "# - calculate LLR\n",
    "# - filter significant bigram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8290c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from chromadb.utils import embedding_functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62f4cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-level elements\n",
    "root.findall(\".\")\n",
    "\n",
    "# All 'neighbor' grand-children of 'country' children of the top-level\n",
    "# elements\n",
    "root.findall(\"./country/neighbor\")\n",
    "\n",
    "# Nodes with name='Singapore' that have a 'year' child\n",
    "root.findall(\".//year/..[@name='Singapore']\")\n",
    "\n",
    "# 'year' nodes that are children of nodes with name='Singapore'\n",
    "root.findall(\".//*[@name='Singapore']/year\")\n",
    "\n",
    "# All 'neighbor' nodes that are the second child of their parent\n",
    "root.findall(\".//neighbor[2]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hug",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
